# AXE - Canada's AI Assistant

Local AI assistant with Claude-quality responses, designed for Canadians.

## Features

- ðŸ§  **Claude-Quality Responses** - Opus 4.5-level extended thinking
- ðŸ‡¨ðŸ‡¦ **Canadian Focus** - PIPEDA compliant, bilingual (EN/FR)
- ðŸ’» **Local Processing** - 100% private, $0 cost
- ðŸš€ **Fast** - Optimized inference with vLLM
- ðŸ”“ **Open Source** - Verifiable and customizable

## Quick Start

### Prerequisites

- Node.js 18+ and Python 3.11+
- 64GB RAM recommended
- Llama 3.3 70B model

### Install

```bash
# Clone repo
git clone https://github.com/memjar/axe-ai-portal.git
cd axe-ai-portal

# Install dependencies
npm install
pip install -r requirements.txt

# Start backend
python api/main.py

# Start frontend (new terminal)
npm run dev
```

Visit `http://localhost:3000`

## Deploy to Vercel

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/memjar/axe-ai-portal)

1. Click button above
2. Connect GitHub repo
3. Set environment variables
4. Deploy!

## Environment Variables

Create `.env.local`:

```
NEXT_PUBLIC_API_URL=http://localhost:8000
MODEL_PATH=/path/to/llama-70b
```

## Project Structure

```
axe-web/
â”œâ”€â”€ app/                  # Next.js app directory
â”‚   â”œâ”€â”€ page.tsx         # Main chat interface
â”‚   â”œâ”€â”€ layout.tsx       # App layout
â”‚   â””â”€â”€ globals.css      # Global styles
â”œâ”€â”€ components/          # React components
â”‚   â”œâ”€â”€ ChatInterface.tsx
â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”œâ”€â”€ ThinkingBlock.tsx
â”‚   â””â”€â”€ InputArea.tsx
â”œâ”€â”€ api/                 # FastAPI backend
â”‚   â”œâ”€â”€ main.py         # API server
â”‚   â”œâ”€â”€ models.py       # Model loading
â”‚   â””â”€â”€ inference.py    # Inference logic
â”œâ”€â”€ public/             # Static assets
â”œâ”€â”€ package.json        # Node dependencies
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ vercel.json        # Vercel config
```

## Tech Stack

- **Frontend:** Next.js 14, React, TypeScript, Tailwind CSS
- **Backend:** FastAPI, vLLM, Llama 3.3 70B
- **Deployment:** Vercel (frontend), Railway/Fly.io (backend)

## License

MIT

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md)

---

ðŸ‡¨ðŸ‡¦ Built in Canada, for Canadians
